# Operating System Concepts
### Enhanced Second-Chance Algorithm
- reference bit / dirty bit(modify bit)
- (0, 0) neither recently used nor modified - best page to replace
- (0, 1) not recently used but modified - not quite as good, because the page will need to be written out before replacement
- (1, 0) recently used but clean - probably will be used again soon
- (1, 1) recently used and modified - probably will be used again soo, and the page will be need to be written out to secondary storage before it can be replaced


---

- 가장 효율적으로 victim을 선정하자. Algorithm
- 93 개의 free frame list -> 93개 다참
- 그러면 page replacement algorithm에 따라 victim을 선정하고 proceeding page replacement



---
#### Allocation of Frames
- Equal vs Proportional


#### Thrashing
- 결국 process의 개수가 많아지면 많아 질수록 main memory의 resource의 한계는 존재한다. 그렇다면 끊임없이 page replacement 및 페이지 교체 및 main memory의 관리를 위한 page in page out을 비롯한 수많은 run time process 들이 존재할 것인데 이 모든 것들이 overhead를 일으킬 것이라는 것.
- CPU의 사용률(Utilization)이 급격하게 내려가는 현상.
- because of page management system
- WORKERS 조합을 만들어라!!!!


# Nginx Architecture
1. Upon startup, an initial set of listening sockets is created.
2. workers then continously accept, read from and write to the sockets while processing HTTP requests and responses.
3. run-loop is important.
4. Non-blocking concept is key feature!!
5. When request is arrived, new process to handle request is not created. 
6. That is the key feature of optimize cpu utilization and low memory usage.
- If there's not enough storage performance to serve disk operations generated by a particular worker,  that worker may still block on reading/writing from disk.
- These modules are http and mail. These two modules provide an additional level of abstraction between the core and lower-level compoennts. 
- In these modules the handling of the sequence of events associated with a respective application layer protocol like HTTP, SMTP OR IMAP is implemented.


# Reactor Pattern

# FAQ
multi-thread based and non-blocking mode multi processs model
!! the resource cost less when thread switching than processs switching.
- With a classic thread-per-connection approach, each socket operates in a blocking mode. 

> **<span style='color:#3867d6'>How does it work?</span>**
> Each network socket in NGINX operates in non-blocking mode.
> Nginx subscribes to these sockets, and the operating system raises an event when one of the sockets is ready for I/O. 
> NGINX can then read or write to that socket without blocking.
> 
> **<span style='color:#3867d6'>How does this differ from a threaded approach?</span>
> With a classic thread-per-connection approach, each socket operates in a blocking mode. The server thread makes a system call and the call blocks if there is no data. The operating system context-switches the thread out, and resumes it when the socket is ready.
> Functionally, the two approaches are equivalent. The threaded approach is easier to implement and debug. However, thread are heavier operating system constructs than epoll events. the overhead of raising an event is lower than the overhead of context-switching threads, and the OS resources necessary to handle an event are much lower than the resources necessary to handle a thread.



